//! This module exports utilities for writing lexers and a token stream type.

use crate::{
    diagnostic::Diagnostics,
    source::{Reader, Source, Span},
};

/// A kind of token, e.g. "if", "number", "identifier", "end of file", etc.
pub trait TokenKind {
    /// Is this an End-Of-File token?
    fn is_eof(&self) -> bool;
}

/// Semantic error returned when a [`Lexer`] fails to produce a token. Actual
/// error should be placed at [`Diagnostics`], this type is just for documenting
/// lexer states, i.e. better to read `Result<Token<K>, LexingError>` instead of
/// `Option<Token<K>>`.
#[derive(Debug, Clone, Copy, Default)]
pub struct LexingError;

/// A token generated by a [`Lexer`].
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub struct Token<K>
where
    K: TokenKind,
{
    /// [`Kind`](TokenKind) of this token, e.g. "identifier", "number",
    /// "semicolon", "while", "end of file", etc.
    pub kind: K,
    /// [`Span`] in the source code of this token.
    pub span: Span,
}

/// A generic lexer.
pub trait Lexer {
    /// [`Token`] classes attached to tokens produced by this lexer.
    type TokenKind: TokenKind;

    /// Attempts to generate a single [`Token`]. The lexer should manipulate the
    /// given [`Reader`] until a token is finished, and then, return the
    /// produced token. Ideally, final [`Reader`]'s position should not end up
    /// behind its initial position (regardig this method call), but if the
    /// lexer can handle not getting stuck, it should be fine.
    ///
    /// Eventually, this method must produce a token whose kind is
    /// [End-Of-File](TokenKind::is_eof). More specifically, EOF token should be
    /// produced when `Reader` is in the end of input indeed. This method must
    /// not be called after EOF token is produced.
    ///
    /// Although this method returns a `Result`, actual lexing errors should be
    /// inserted on [`Diagnostics`] via [`Diagnostics::raise`].  [`LexingError`]
    /// is just a semantic error for avoiding meaningless `Option`s (i.e. better
    /// reading `Result<Token<K>, LexingError>` than `Option<Token<K>>`). Note
    /// that it is expected that when [`LexingError`] is returned, at least
    /// one error is placed on [`Diagnostics`].
    fn generate_token(
        &mut self,
        reader: &mut Reader,
        diagnostics: &mut Diagnostics,
    ) -> Result<Token<Self::TokenKind>, LexingError>;
}

impl<'this, L> Lexer for &'this mut L
where
    L: Lexer + ?Sized,
{
    type TokenKind = L::TokenKind;

    fn generate_token(
        &mut self,
        reader: &mut Reader,
        diagnostics: &mut Diagnostics,
    ) -> Result<Token<Self::TokenKind>, LexingError> {
        (**self).generate_token(reader, diagnostics)
    }
}

impl<L> Lexer for Box<L>
where
    L: Lexer + ?Sized,
{
    type TokenKind = L::TokenKind;

    fn generate_token(
        &mut self,
        reader: &mut Reader,
        diagnostics: &mut Diagnostics,
    ) -> Result<Token<Self::TokenKind>, LexingError> {
        (**self).generate_token(reader, diagnostics)
    }
}

/// A stream of [`Token`]s, where tokens are generated lazily, but everything is
/// saved. Uses a generic [`Lexer`] to generate tokens.
#[derive(Debug, Clone)]
pub struct TokenStream<L>
where
    L: Lexer,
{
    reader: Reader,
    lexer: L,
    tokens: Vec<Result<Token<L::TokenKind>, LexingError>>,
    position: usize,
}

impl<L> TokenStream<L>
where
    L: Lexer,
{
    /// Creates a token stream from source code, custom lexer, and diagnostics
    /// collection where errors will be placed.
    pub fn new(
        source: &Source,
        lexer: L,
        diagnostics: &mut Diagnostics,
    ) -> Self {
        let mut this = Self {
            reader: source.reader(),
            lexer,
            tokens: Vec::new(),
            position: 0,
        };
        let tok_res = this.lexer.generate_token(&mut this.reader, diagnostics);
        this.tokens.push(tok_res);
        this
    }

    /// Returns whether the End-Of-File has been reached. More specifically,
    /// tests if current token is an EOF token.
    pub fn is_eof(&self) -> bool {
        self.current().map_or(false, |token| token.kind.is_eof())
    }

    /// Returns the position of the current token.
    pub fn position(&self) -> usize {
        self.position
    }

    /// Returns a reference to the current [`Token`]. If the current token could
    /// not be generated, and instead an error occured during its generation,
    /// [`LexingError`] is returned, but actual errors were placed in
    /// [`Diagnostics`].
    pub fn current(&self) -> Result<&Token<L::TokenKind>, LexingError> {
        self.tokens[self.position].as_ref().map_err(|error| *error)
    }

    /// Returns the underlying source code object.
    pub fn source(&self) -> &Source {
        self.reader.source()
    }

    /// Advances the stream's position by one, possibly generating a new token,
    /// given a diagnostics collection where errors will be placed. Returns
    /// whether the stream does have a next positon (and thus advanced).
    pub fn next(&mut self, diagnostics: &mut Diagnostics) -> bool {
        if self.is_eof() {
            false
        } else {
            self.position += 1;
            if self.position >= self.tokens.len() {
                let tok_res =
                    self.lexer.generate_token(&mut self.reader, diagnostics);
                self.tokens.push(tok_res);
            }
            true
        }
    }

    /// Goes back by one token in the stream. Returns whether the stream
    /// actually went back. Note that it will not go back only if already at
    /// position `0`.
    pub fn prev(&mut self) -> bool {
        if self.position == 0 {
            false
        } else {
            self.position -= 1;
            true
        }
    }

    /// Advances the stream's position by `count` positions, possibly generating
    /// new tokens, given a diagnostics collection where errors will be placed.
    /// Returns how many positions were actually advanced (it won't advance past
    /// beyond the end of the stream).
    pub fn advance(
        &mut self,
        count: usize,
        diagnostics: &mut Diagnostics,
    ) -> usize {
        let mut advanced = count.min(self.tokens.len() - self.position);
        self.position += advanced;
        while advanced < count && self.next(diagnostics) {
            advanced += 1;
        }
        advanced
    }

    /// Rollsback stream position by `count` positions, and returns how many
    /// positions were actually back. Note that it will stop if position `0`
    /// will be reached.
    pub fn rollback(&mut self, count: usize) -> usize {
        let rolled = count.min(self.position);
        self.position -= rolled;
        rolled
    }
}
